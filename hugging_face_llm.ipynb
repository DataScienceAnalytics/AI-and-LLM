{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369f1182-8c86-46bc-84fa-bf87d083e384",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b420be37-6a61-44b5-bbcf-4b5e9d13a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89f7fe9c-fe45-47bd-859d-fce181e5d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d07a37-3d65-4c1e-9dd3-aa22690ac347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa67d245-1c8e-40cc-b925-6ec51964a3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a384e805-ae31-4ece-8fa3-3116fec13a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using  cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"using \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19017606-fe9d-4283-ae1f-3c915988999a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f29ffe-4096-42a3-9188-4931a3b48f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7305226-2f25-4fa7-85ff-5a291c37fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ac31e56-47f0-4d68-960b-4fbb3df5516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"lmsys/fastchat-t5-3b-v1.0\"\n",
    "filenames = [\n",
    "        \"pytorch_model.bin\", \"added_tokens.json\", \"config.json\", \"generation_config.json\", \n",
    "        \"special_tokens_map.json\", \"spiece.model\", \"tokenizer_config.json\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ac07e012-bcb7-4d31-ae32-d8e6cc19c772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#download necessary files\n",
    "# for filename in filenames:\n",
    "#         downloaded_model_path = hf_hub_download(\n",
    "#                     repo_id=model_id,\n",
    "#                     filename=filename,\n",
    "#                     token=HUGGING_FACE_API_KEY\n",
    "#         )\n",
    "#         print(downloaded_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49d7d6-11fc-45b2-898d-db19e2f57a64",
   "metadata": {},
   "source": [
    "## Run LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f166603b-9869-4ebb-8aab-dff06fadc6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded successfully.\n",
      "Model loaded successfully.\n",
      "Pipeline created successfully.\n",
      "Pipeline output: [{'generated_text': \"Comment faites-vous aujourd'hui?\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Use a known model to debug\n",
    "model_id = \"t5-small\"\n",
    "\n",
    "try:\n",
    "    # Ensure sentencepiece is installed\n",
    "    import sentencepiece\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    print(\"Tokenizer loaded successfully.\")\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    print(\"Model loaded successfully.\")\n",
    "\n",
    "    # Create pipeline\n",
    "    nlp_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer, device=-1, max_length=1000)\n",
    "    print(\"Pipeline created successfully.\")\n",
    "\n",
    "    # Test the pipeline with a sample input\n",
    "    result = nlp_pipeline(\"Translate English to French: Hi, how are you doing today?\")\n",
    "    print(\"Pipeline output:\", result)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error:\", str(e))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e968fefa-5561-45c7-86bd-e32fa3f11ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, legacy=False)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "\n",
    "pipeline = pipeline(\"text2text-generation\", model=model, device=-1, tokenizer=tokenizer, max_length=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42375ffc-9792-4ed9-9a3d-19780afa345e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '<pad> og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og r og'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(\"Capital of New Zealand\")\n",
    "# Doesn't seems to work properly here but the translation task above works quite well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea5725-d026-47f8-b8bc-035e176bc319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
